{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.svm import SVC\n",
    "import graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "import os\n",
    "os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files (x86)/Graphviz2.38/bin/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading dataset 2011\n",
    "df1 = pd.read_csv(\"Violencia_Domestica_2011.csv\",sep=';',keep_default_na=True).rename(index=str,columns={'P07':'P08','P151':'P071','P152':'P072','P153':'P073','P154':'P074','P151':'P071','P32':'P31', 'P31':'P30', 'P30':'P29', 'P29':'P28', 'P28':'P27', 'P27':'P26', 'P26':'P25', 'P25':'P24', 'P24':'P23', 'P23':'P22', 'P227':'P217', 'P226':'P216', 'P225':'P215', 'P224':'P214', 'P223':'P213', 'P222':'P212', 'P221':'P211', 'P2111':'P2011', 'P2110':'P2010', 'P2109':'P2009', 'P2108':'P2008', 'P2107':'P2007', 'P2106':'P2006', 'P2105':'P2005', 'P2104':'P2004', 'P2103':'P2003', 'P2102':'P2002', 'P2101':'P2001', 'P20':'P19', 'P197':'P187', 'P196':'P186', 'P195':'P185', 'P194':'P184', 'P193':'P183', 'P192':'P182', 'P191':'P181', 'P18':'P17', 'P17':'P16', 'P16':'P15'}\n",
    ").replace(' ',-2).drop(['P14','P33_1','P33_2','N_PESSOAS'],axis=1)\n",
    "# agression_2011 = df[df['P19'] == 1]\n",
    "# non_agression_2011 = df[df['P19'] == 2]\n",
    "\n",
    "# Loading dataset 2013\n",
    "df2 = pd.read_csv(\"Violencia_Domestica_2013.csv\",sep=';').drop(['P14','N_PESSOAS'],axis=1)\n",
    "# agression_2013 = df[df['P19'] == 1]\n",
    "# non_agression_2013 = df[df['P19'] == 2]\n",
    "\n",
    "# Loading dataset 2015\n",
    "df3 = pd.read_csv(\"Violencia_Domestica_2015.csv\",sep=';', keep_default_na=True).drop(['P14','PESO'],axis=1).replace(' ',-2)\n",
    "# df = pd.read_csv(\"Violencia_Domestica_2015.csv\",sep=';')\n",
    "# agression_2015 = df[df['P19'] == 1]\n",
    "# non_agression_2015 = df[df['P19'] == 2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the three databases\n",
    "merged = pd.concat([df1,df2,df3],axis=0,sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping questions\n",
    "cols = [name for name in  merged.columns if name[0] == 'P' and name != 'P19']\n",
    "\n",
    "# dropping year\n",
    "clean_merged = merged.drop(cols, axis=1).drop(['ANO'],axis=1)\n",
    "\n",
    "# dropping instances with missing values\n",
    "clean_merged = clean_merged[ clean_merged['RACACOR'] != 99 ]\n",
    "clean_merged = clean_merged[ clean_merged['RENDA'] != 99 ]\n",
    "clean_merged = clean_merged[ clean_merged['ESC'] != 99 ]\n",
    "clean_merged = clean_merged[ clean_merged['IDADE'] != 99 ]\n",
    "clean_merged = clean_merged[ clean_merged['SEXO'] != 99 ]\n",
    "#Esc = Escolaridade. Ordinal\n",
    "#Idade. Ordinal\n",
    "#P19 = target\n",
    "#RACACOR = Categórico\n",
    "#Regiao = categorico\n",
    "#RENDA = ordinal\n",
    "#Sexo = Categórico\n",
    "#UF = Categórico\n",
    "\n",
    "\n",
    "# Adding dummy variables to categorical attributes\n",
    "dummy_merged = pd.get_dummies(clean_merged, columns=['RACACOR','REGIAO','SEXO','UF'])\n",
    "\n",
    "# splitting positive and negative instances (positive means this woman has suffered domestic violence)\n",
    "positive = dummy_merged[ dummy_merged['P19'] == 1]\n",
    "negative = dummy_merged[ dummy_merged['P19'] == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5834, 43)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Datasets are unbalanced. I'll try to fix this by undersampling\n",
    "\n",
    "# Oversampling\n",
    "sampled_positive = positive.sample(frac=negative.shape[0]/positive.shape[0],replace=1)\n",
    "sampled_negative = negative.sample(frac=1)\n",
    "\n",
    "# Undersampling\n",
    "# sampled_positive = positive.sample(frac=1)\n",
    "# sampled_negative = negative.sample(frac=positive.shape[0]/negative.shape[0])\n",
    "\n",
    "merged_sampled = pd.concat([sampled_positive, sampled_negative])\n",
    "merged_sampled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot a RandomForest tree \n",
    "def plot_tree(estimator, feature_names = [name for name in  merged_sampled.columns if name != 'P19'], class_names = ['Violented','Not Violented']):\n",
    "    dot_data = export_graphviz(estimator, out_file='tree.dot', \n",
    "                    feature_names = feature_names,\n",
    "                    class_names = class_names,\n",
    "                    rounded = True, proportion = False, \n",
    "                    precision = 2, filled = True)\n",
    "\n",
    "    # Convert to png using system command (requires Graphviz)\n",
    "    from subprocess import call\n",
    "    call(['dot', '-Tpng', 'tree.dot', '-o', 'tree.png', '-Gdpi=600'])\n",
    "\n",
    "    # Display in jupyter notebook\n",
    "    from IPython.display import Image\n",
    "    Image(filename = 'tree.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.73      0.87      0.79       973\n",
      "         1.0       0.84      0.68      0.75       973\n",
      "\n",
      "   micro avg       0.77      0.77      0.77      1946\n",
      "   macro avg       0.78      0.77      0.77      1946\n",
      "weighted avg       0.78      0.77      0.77      1946\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.75      0.89      0.81       972\n",
      "         1.0       0.86      0.70      0.77       972\n",
      "\n",
      "   micro avg       0.79      0.79      0.79      1944\n",
      "   macro avg       0.81      0.79      0.79      1944\n",
      "weighted avg       0.81      0.79      0.79      1944\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.72      0.87      0.79       972\n",
      "         1.0       0.84      0.66      0.74       972\n",
      "\n",
      "   micro avg       0.76      0.76      0.76      1944\n",
      "   macro avg       0.78      0.76      0.76      1944\n",
      "weighted avg       0.78      0.76      0.76      1944\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\rafae\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Normalizing values for machine learning\n",
    "scaler = MinMaxScaler()\n",
    "# Scaling data using scikit\n",
    "scaled_data = scaler.fit_transform(merged_sampled.astype(float))\n",
    "\n",
    "merged_sampled.iloc[0]\n",
    "\n",
    "# # Y = target, X = \n",
    "Y = scaled_data[:,2]\n",
    "X = np.delete(scaled_data,2, axis=1)\n",
    "\n",
    "# Creating the classifier\n",
    "clf = RandomForestClassifier()\n",
    "\n",
    "# Creating a k-fold iterator\n",
    "skf = StratifiedKFold(n_splits=3)\n",
    "\n",
    "# For each fold\n",
    "for train, test in skf.split(X,Y):\n",
    "    # trains the model\n",
    "    clf.fit(X[train],Y[train])\n",
    "    # prints the classification report for this fold\n",
    "    print(classification_report(Y[test],clf.predict(X[test])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training model on the whole dataset\n",
    "clf = clf.fit(X,Y)\n",
    "\n",
    "from pickle import load, dump\n",
    "\n",
    "# Saving model\n",
    "with open('model.pkl','wb') as f:\n",
    "    dump(clf, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the trained model\n",
    "\n",
    "# loading model\n",
    "with open('model.pkl','rb') as f:\n",
    "    clf = load(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
